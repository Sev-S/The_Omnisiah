### Inverse Matrizen
Definition:
Sei $A$ quadratisch, es gelte: $B\cdot A=A\cdot B=E$, dann gilt: $B$ ist invers zu $A$, $B=A^{-1}$ ($A$ regulär, $A$ invertierbar, $A^{-1}$ existiert)

6.11c) $(A\cdot B)^{-1}=B^{-1}\cdot A^{-1}$ Behauptung!
	Überprüfe:
	$(A\cdot B)\cdot(B^{-1}\cdot A^{-1})=(B^{-1}\cdot A^{-1})\cdot (A\cdot B)=E$?
	Beweis:
	$(A\cdot B)\cdot(B^{-1}\cdot A^{-1})=A\cdot (B\cdot B^{-1})\cdot A^{-1}=A\cdot E\cdot A^{-1}\ \checkmark$
	$(B^{-1}\cdot A^{-1})\cdot  (A\cdot B)=B^{-1}\cdot (A^{-1}\cdot A)\cdot B=B^{-1}\cdot E\cdot B=E\ \checkmark$
	$\implies$ also gilt: $(A\cdot B)$ ist invertierbar und $(A\cdot B)^{-1}=B^{-1}\cdot A^{-1}$
A ist invertierbar $\Leftrightarrow \det A\neq 0$
$\det (A\cdot B)=\det A\cdot\det B$

*$|\cdot (A\cdot B)$ Immer dran schreiben von welcher Seite man dranmultipliziert, sonst nicht äquivalent*

Besser nie mit der Behauptung starten ,um etwas zu zeigen, sehr fehleranfällig

### Matrixmultiplikation
6.10)
a) $A\cdot(B\cdot C)=(A\cdot B)\cdot C$
	$A\cdot(B\cdot C)=:D$
	$(A\cdot B)\cdot C=:\tilde{D}$
	$D_{ik}=\sum_{l}A_{il}\cdot \left( \sum_{j}B_{lj}\cdot C_{jk} \right)$
	alternativ: $d_{ik}=\sum_{l}a_{il}\cdot (\sum_{j}b_{lj}\cdot c_{jk}=\sum_{l}\sum_{j}a_{il}\cdot b_{lj}\cdot c_{jk}=\dots$
e) $(A\cdot B)^T=B^T\cdot A^T$
	$A=(a_{ij}),\ A^T=(\alpha_{ji}),\ \alpha_{ji}=\alpha_{ij}$
	$B=(b_{ij}),\ B^T=(\beta_{ji}),\ \beta_{ji}=\beta_{ij}$
	$(A\cdot B)^T=\left( \sum_{k=1}^na_{jk}\cdot b_{kj} \right)^T= \sum_{k=1}^na_{jk}\cdot b_{ki}=\sum_{k=1}^nb_{ki}\cdot a_{jk}=\sum_{k=1}^n\beta_{ik}\cdot \alpha_{kj}=B^T\cdot A^T$

### Skalarprodukt auf dem Vektorraum der Matrizen $M(m\times n)$ (quadratisch)
6.9c)
$<A,B>:=Spur(A^T\cdot B)=Spur\left( \sum_{j}a_{ij}\cdot b_{ik} \right)=\sum_{i}\sum_{j}a_{ji}\cdot b_{ji}$
$Spur(M)=\sum_{i=1}^nm_{ii}\in \mathbb{R}$ Summe der Diagonalelemente $Spur(A^T)=Spur(A)$
$<B,A>:=Spur(B^T\cdot A)=\sum_{i}\sum_{j}b_{ij}\cdot a_{ji}=\sum_{i}\sum_{j}a_{ji}\cdot c_{ji}$
positiv definit:
$<A,A>:=\sum_{i}\sum_{j}a_{ij}\cdot a_{ij}(a_{ij})^2\geq_{0}$
für $A\neq 0$, also nicht alle $a_{ij}=0\implies<A,A>\ >0$
Beispiel: $\sum_{i=1}^3\sum_{j=1}^3(a_{ij})^2=a_{11}^2+a_{21}^2+a_{31}^2+a_{12}^2+a_{22}^2+a_{23}^1+\dots\ \begin{pmatrix}a_{11} & a_{12} & a_{13}\\ a_{21} & a_{22} & a_{23}\\ a_{31} & a_{32} & a_{33}\end{pmatrix}$

### Orthogonale Matrizen
$A^T\cdot A=E$, wenn $A$ orthogonal
$\begin{pmatrix}\cdot & \cdot & \cdot\\ \cdot & \cdot & \cdot\\ \cdot & \cdot & \cdot\end{pmatrix} \cdot \begin{pmatrix}\cdot & \cdot & \cdot\\ \cdot & \cdot & \cdot\\ \cdot & \cdot & \cdot\end{pmatrix}=\begin{pmatrix}1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 1\end{pmatrix}$ -> Vektoren bilden Orthonormalbasis von $\mathbb{R}^3$
$A^T=A$
Drehmatrix im $\mathbb{R}^2$ $D_{(\alpha)}=\begin{pmatrix}\cos(\alpha) & -\sin(\alpha)\\ \\ \sin(\alpha) & \cos(\alpha)\end{pmatrix}\ |D_{(\alpha)}|=\cos^2+\sin^2=1$
a) $D_{(\alpha)}^{-1}=D_{(\alpha)}^T$
c) $D_{(\alpha)}=D_{(\alpha)}^{-1}$
$D_{(\alpha)}\cdot D_{(\beta)}=D_{(\alpha+\beta)}\Leftarrow$ Additionstheoreme